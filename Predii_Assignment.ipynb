{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymupdf langchain langchain-openai faiss-cpu gradio pydantic nest_asyncio"
      ],
      "metadata": {
        "id": "AhTSDeiHtWw6"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import traceback\n",
        "import fitz\n",
        "import gradio as gr\n",
        "import nest_asyncio\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "OPENAI_API_KEY = \"OPENAI_API_KEY"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "class VehicleSpec(BaseModel):\n",
        "    component: str = Field(description=\"e.g., Brake Caliper Bolt\")\n",
        "    spec_type: str = Field(description=\"e.g., Torque\")\n",
        "    value: str = Field(description=\"e.g., 35\")\n",
        "    unit: str = Field(description=\"e.g., Nm\")\n",
        "\n",
        "class VehicleSpecList(BaseModel):\n",
        "    specifications: List[VehicleSpec]\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self):\n",
        "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "        self.llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "        self.vectorstore = None\n",
        "        self.current_pdf_name = None\n",
        "        self.structured_llm = self.llm.with_structured_output(VehicleSpecList)\n",
        "\n",
        "    def process_document(self, file_path: str):\n",
        "        if self.current_pdf_name == file_path:\n",
        "            return\n",
        "\n",
        "        doc = fitz.open(file_path)\n",
        "        raw_text = \"\"\n",
        "\n",
        "        for page in doc:\n",
        "            raw_text += page.get_text(\"text\") + \"\\n\\n\"\n",
        "\n",
        "        chunks = self.text_splitter.split_text(raw_text)\n",
        "        documents = [Document(page_content=chunk) for chunk in chunks]\n",
        "\n",
        "        self.vectorstore = FAISS.from_documents(documents, self.embeddings)\n",
        "        self.current_pdf_name = file_path\n",
        "\n",
        "    def extract_specs(self, query: str):\n",
        "        if not self.vectorstore:\n",
        "            return json.dumps({\"error\": \"System not initialized with a document.\"})\n",
        "\n",
        "        retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "        docs = retriever.invoke(query)\n",
        "        context = \"\\n---\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        You are an expert automotive system.\n",
        "        Extract the requested vehicle specifications from the provided context only.\n",
        "        If the context does not contain the answer, return an empty list.\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        QUERY:\n",
        "        {query}\n",
        "        \"\"\")\n",
        "\n",
        "        chain = prompt | self.structured_llm\n",
        "        result = chain.invoke({\"context\": context, \"query\": query})\n",
        "\n",
        "        return json.dumps([spec.model_dump() for spec in result.specifications], indent=4)\n",
        "\n",
        "pipeline = RAGPipeline()\n",
        "\n",
        "def handle_request(pdf_file, query):\n",
        "    if not pdf_file:\n",
        "        return json.dumps({\"error\": \"Please upload a PDF file.\"})\n",
        "    if not query:\n",
        "        return json.dumps({\"error\": \"Please enter a query.\"})\n",
        "\n",
        "    try:\n",
        "        file_path = pdf_file if isinstance(pdf_file, str) else pdf_file.name\n",
        "\n",
        "        pipeline.process_document(file_path)\n",
        "        result = pipeline.extract_specs(query)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = {\"error\": str(e), \"traceback\": traceback.format_exc()}\n",
        "        return json.dumps(error_msg, indent=4)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=handle_request,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload Service Manual (PDF)\", file_count=\"single\"),\n",
        "        gr.Textbox(label=\"Enter Query (e.g., 'Torque for brake caliper bolts')\")\n",
        "    ],\n",
        "    outputs=gr.Code(label=\"Extracted Structured Data\", language=\"json\"),\n",
        "    title=\"Vehicle Specification Extraction\",\n",
        "    description=\"Upload a PDF and enter a query to extract structured specifications.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "demo.queue()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "5ilbIFGRtT7z",
        "outputId": "ef4f192d-5dfd-42d0-fa60-59d956a12781"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3c7eb6855af34364e0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3c7eb6855af34364e0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
